{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>CBOW: context -> center</p>\n",
    "<p>Skip-Gram: center -> context</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A Toy Example (using the skip-gram model)</h2>\n",
    "<p>Assuming: </p>\n",
    "<ul>\n",
    "    <li>vocabulary of 8</li>\n",
    "    <li>word-representation vectors of size 6 (i.e 6x1)</li>\n",
    "    <li>scaling our outputs using a softmax function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([[\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]])\n",
    "embedding_length = 6\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / np.sum(e_z, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set parameters and generate training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.]])]\n",
      " [array([1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.]])]\n",
      " [array([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "  array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.]])]\n",
      " [array([0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "  array([[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "  array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.]])]]\n"
     ]
    }
   ],
   "source": [
    "# Word counts\n",
    "wd_counts = defaultdict(int)\n",
    "for row in corpus:\n",
    "    for word in row:\n",
    "        wd_counts[word] += 1\n",
    "\n",
    "# Set parameters\n",
    "# Size of vocabulary\n",
    "V = len(wd_counts.keys())\n",
    "# Length of word vector embeddings\n",
    "N = embedding_length\n",
    "# Context window size (i.e. n words on either side of target word)\n",
    "C = 2\n",
    "# Learning rate for the network\n",
    "ETA = 0.1\n",
    "\n",
    "# Look-up tables\n",
    "vocabulary = sorted(list(wd_counts.keys()))\n",
    "word_index = dict((word, i) for i, word in enumerate(vocabulary))\n",
    "index_word = dict((i, word) for i, word in enumerate(vocabulary))\n",
    "\n",
    "def one_hot(word):\n",
    "    word_vec = np.zeros(shape=(V,))\n",
    "    word_vec[word_index[word]] = 1\n",
    "    return word_vec\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    sentence_len = len(sentence)\n",
    "        \n",
    "    for i, word in enumerate(sentence):\n",
    "        w_target = one_hot(word)\n",
    "        w_context = []\n",
    "        \n",
    "        for j in range(i - C, i + C + 1):\n",
    "            if j >= 0 and j != i and j < len(sentence):\n",
    "                w_context.append(one_hot(sentence[j]))\n",
    "        training_data.append([w_target, np.array(w_context)])\n",
    "                \n",
    "training_data = np.array(training_data)\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initializing values</h3>\n",
    "<p>Assuming word 3 is our target (center word)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "yc:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "W1:\n",
      "[[ 0.08614698 -0.36750216 -0.33938485  0.43620256 -0.26515335  0.18553837]\n",
      " [ 0.12985622  0.04285785  0.15063536  0.37230336  0.12039416 -0.22919682]\n",
      " [ 0.17431804  0.49787207  0.215355    0.02048259 -0.14082999  0.13227316]\n",
      " [-0.20620978 -0.37244012  0.01139629  0.49346479  0.41051038  0.01246903]\n",
      " [ 0.26771751 -0.14853589  0.44364546 -0.21972808  0.18541168 -0.13683058]\n",
      " [-0.25515609  0.43829243 -0.41089684  0.28826452 -0.313391    0.42865522]\n",
      " [-0.24822564  0.22069793  0.17458605 -0.38330372  0.47706552 -0.03015181]\n",
      " [ 0.14855631 -0.47574165 -0.01324896 -0.10695233 -0.16533702  0.37926315]]\n",
      "W2:\n",
      "[[-0.13743996  0.18054415  0.38884914 -0.37203156 -0.31430562  0.33042897\n",
      "   0.06733084 -0.0082668 ]\n",
      " [-0.20449573  0.27514662 -0.40266812  0.04738372  0.0676584  -0.10378056\n",
      "  -0.14718391  0.34350396]\n",
      " [-0.40257345 -0.16245016  0.48960491 -0.00752228 -0.09404276  0.2998467\n",
      "   0.16988263  0.33221779]\n",
      " [ 0.07504743 -0.12265215 -0.2012323  -0.01452417  0.01380635 -0.40172526\n",
      "  -0.03021565  0.28434279]\n",
      " [ 0.30286621  0.03512344  0.39061385  0.09933494  0.13847295 -0.34500894\n",
      "  -0.46381347  0.4379496 ]\n",
      " [-0.11877511 -0.04926574  0.07932022  0.05591236  0.47281597 -0.22889517\n",
      "   0.46452206  0.48348952]]\n"
     ]
    }
   ],
   "source": [
    "# x -> a one-hot encoding of our word of interest (\"brown\" in this case)\n",
    "x = training_data[2][0]\n",
    "Yc = training_data[2][1]\n",
    "\n",
    "# W -> the word vectors for each word in our vocabulary\n",
    "W1 = np.random.uniform(low=-0.5, high=0.5, size=(V,N))\n",
    "W2 = np.random.uniform(low=-0.5, high=0.5, size=(N,V))\n",
    "\n",
    "print(\"x:\\n{}\".format(x))\n",
    "print(\"yc:\\n{}\".format(Yc))\n",
    "print(\"W1:\\n{}\".format(W1))\n",
    "print(\"W2:\\n{}\".format(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Forward Pass: Getting Through the network</h3>\n",
    "<ol>\n",
    "    <li>Use one-hot encoding to extract word vector of interest (h)</li>\n",
    "    <li>Find scores for the output (i.e. scores that represent confidence of window co-occurence with the center word</li>\n",
    "    <li>Pass the scores through the softmax function to compress them to \"probabilities\" (values in the range [0, 1])</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\n",
      "[ 0.08614698 -0.36750216 -0.33938485  0.43620256 -0.26515335  0.18553837]\n",
      "u:\n",
      "[ 0.13033247 -0.10238548 -0.16131858 -0.06921065  0.0370069  -0.16138054\n",
      "  0.19822338 -0.14208718]\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "h = np.dot(x, W1)\n",
    "print(\"h:\\n{}\".format(h))\n",
    "\n",
    "# Step 2\n",
    "u = np.dot(W2.T, h)\n",
    "print(\"u:\\n{}\".format(u))\n",
    "\n",
    "y = softmax(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Error calculation and backpropagation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "[ 0.5840973   0.46282529 -0.56366228 -0.52156304  0.53205242  0.43631069\n",
      " -0.37487071 -0.55518968]\n",
      "Deltas for W2:\n",
      "[[ 0.05031822  0.039871   -0.0485578  -0.04493108  0.04583471  0.03758685\n",
      "  -0.03229398 -0.04782791]\n",
      " [-0.21465702 -0.17008929  0.2071471   0.19167554 -0.19553041 -0.16034512\n",
      "   0.13776579  0.20403341]\n",
      " [-0.19823377 -0.15707589  0.19129844  0.17701059 -0.18057053 -0.14807724\n",
      "   0.12722544  0.18842297]\n",
      " [ 0.25478474  0.20188558 -0.24587093 -0.22750713  0.23208263  0.19031984\n",
      "  -0.16351956 -0.24217516]\n",
      " [-0.15487536 -0.12271968  0.14945694  0.13829419 -0.14107548 -0.11568924\n",
      "   0.09939822  0.1472104 ]\n",
      " [ 0.10837246  0.08587185 -0.10458098 -0.09676996  0.09871614  0.08095238\n",
      "  -0.0695529  -0.10300899]]\n",
      "Deltas for W1:\n",
      "[[-0.06556768  0.06533697 -0.74971385 -0.20639773 -0.22495561 -0.45691992]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.         -0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "EI = np.sum([np.subtract(y, yc) for yc in Yc], axis = 0)\n",
    "print(\"Error: \\n{}\".format(EI))\n",
    "\n",
    "# Calculate the deltas\n",
    "dl_dW2 = np.outer(h, EI)\n",
    "print(\"Deltas for W2:\\n{}\".format(dl_dW2))\n",
    "# Note: updates for W1 will only have one row of non-zero values:\n",
    "# That means that only the word vector for \"brown\" (i.e. W1[0]) has been updated -- which is what we expected\n",
    "dl_dW1 = np.outer(x, np.dot(W2, EI.T))\n",
    "print(\"Deltas for W1:\\n{}\".format(dl_dW1))\n",
    "\n",
    "# Update the weights \n",
    "W1 -= ETA * dl_dW1\n",
    "W2 -= ETA * dl_dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
