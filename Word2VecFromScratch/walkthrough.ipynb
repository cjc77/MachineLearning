{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>CBOW: context -> center</p>\n",
    "<p>Skip-Gram: center -> context</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A Toy Example (using the skip-gram model)</h2>\n",
    "<p>Assuming: </p>\n",
    "<ul>\n",
    "    <li>vocabulary of 8</li>\n",
    "    <li>word-representation vectors of size 6 (i.e 6x1)</li>\n",
    "    <li>scaling our outputs using a softmax function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([[\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]])\n",
    "embedding_length = 6\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / np.sum(e_z, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set parameters and generate training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.]])]\n",
      " [array([1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.]])]\n",
      " [array([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "  array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.]])]\n",
      " [array([0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "  array([[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "  array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0.]])]\n",
      " [array([0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "  array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0.]])]]\n",
      "(9, 2)\n"
     ]
    }
   ],
   "source": [
    "# Word counts\n",
    "wd_counts = defaultdict(int)\n",
    "for row in corpus:\n",
    "    for word in row:\n",
    "        wd_counts[word] += 1\n",
    "\n",
    "# Set parameters\n",
    "# Size of vocabulary\n",
    "V = len(wd_counts.keys())\n",
    "# Length of word vector embeddings\n",
    "N = embedding_length\n",
    "# Context window size (i.e. n words on either side of target word)\n",
    "C = 2\n",
    "# Learning rate for the network\n",
    "ETA = 0.1\n",
    "\n",
    "# Look-up tables\n",
    "vocabulary = sorted(list(wd_counts.keys()))\n",
    "word_index = dict((word, i) for i, word in enumerate(vocabulary))\n",
    "index_word = dict((i, word) for i, word in enumerate(vocabulary))\n",
    "\n",
    "def one_hot(word):\n",
    "    word_vec = np.zeros(shape=(V,))\n",
    "    word_vec[word_index[word]] = 1\n",
    "    return word_vec\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    sentence_len = len(sentence)\n",
    "        \n",
    "    for i, word in enumerate(sentence):\n",
    "        w_target = one_hot(word)\n",
    "        w_context = []\n",
    "        \n",
    "        for j in range(i - C, i + C + 1):\n",
    "            if j >= 0 and j != i and j < sentence_len:\n",
    "                w_context.append(one_hot(sentence[j]))\n",
    "        training_data.append([w_target, np.array(w_context)])\n",
    "                \n",
    "training_data = np.array(training_data)\n",
    "\n",
    "print(training_data)\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initializing values</h3>\n",
    "<p>Assuming word 3 is our target (center word)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "yc:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "W1:\n",
      "[[-0.05133965  0.31932449 -0.00076711 -0.31183346 -0.06962123 -0.04505802]\n",
      " [ 0.14988032 -0.23083012 -0.11909168 -0.49174947  0.49448661  0.46887054]\n",
      " [ 0.33277589  0.05662652  0.2674883   0.19789967  0.02792778 -0.22627179]\n",
      " [-0.02431183 -0.4039943   0.38077218 -0.01073757  0.41596685 -0.37685937]\n",
      " [ 0.39006347  0.47808053 -0.24210394  0.30791468  0.27615354  0.38503764]\n",
      " [ 0.2705183   0.43405893  0.10413934 -0.34114987 -0.26725919  0.4783493 ]\n",
      " [-0.07697521  0.20290811  0.23088942 -0.37217582  0.21298191  0.15616153]\n",
      " [-0.12239315 -0.48511851  0.21738647 -0.01070983  0.12328203  0.45453927]]\n",
      "W2:\n",
      "[[ 0.19396665 -0.27469498 -0.39012654  0.00415826 -0.48835719  0.44879869\n",
      "   0.09504474 -0.1669008 ]\n",
      " [-0.3625185   0.47701613 -0.42872927 -0.10263754 -0.14334517  0.37864322\n",
      "  -0.09233567 -0.19690589]\n",
      " [-0.15118394 -0.01370951  0.31048555 -0.44533149  0.48563386 -0.10220793\n",
      "  -0.04726353 -0.47660161]\n",
      " [ 0.00854882  0.32274336 -0.37054656  0.47051659  0.10053912 -0.45359788\n",
      "  -0.13494071 -0.09075673]\n",
      " [ 0.10958777 -0.49988148 -0.22750208  0.38495305  0.34934218 -0.33121546\n",
      "   0.31498523 -0.08579507]\n",
      " [-0.43856824 -0.34758426  0.4936066  -0.35040158  0.34463931  0.08072099\n",
      "  -0.39306645  0.22715294]]\n"
     ]
    }
   ],
   "source": [
    "# x -> a one-hot encoding of our word of interest (\"brown\" in this case)\n",
    "x = training_data[0][0]\n",
    "Yc = training_data[0][1]\n",
    "\n",
    "# W -> the word vectors for each word in our vocabulary\n",
    "W1 = np.random.uniform(low=-0.5, high=0.5, size=(V,N))\n",
    "W2 = np.random.uniform(low=-0.5, high=0.5, size=(N,V))\n",
    "\n",
    "print(\"x:\\n{}\".format(x))\n",
    "print(\"yc:\\n{}\".format(Yc))\n",
    "print(\"W1:\\n{}\".format(W1))\n",
    "print(\"W2:\\n{}\".format(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Forward Pass: Getting Through the network</h3>\n",
    "<ol>\n",
    "    <li>Use one-hot encoding to extract word vector of interest (h)</li>\n",
    "    <li>Find scores for the output (i.e. scores that represent confidence of window co-occurence with the center word</li>\n",
    "    <li>Pass the scores through the softmax function to compress them to \"probabilities\" (values in the range [0, 1])</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\n",
      "[-0.12239315 -0.48511851  0.21738647 -0.01070983  0.12328203  0.45453927]\n",
      "u:\n",
      "[-0.06666894 -0.42384246  0.52351384 -0.16437925  0.43352415 -0.26011944\n",
      " -0.11550046  0.1059884 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "h = np.dot(x, W1)\n",
    "print(\"h:\\n{}\".format(h))\n",
    "\n",
    "# Step 2\n",
    "u = np.dot(W2.T, h)\n",
    "print(\"u:\\n{}\".format(u))\n",
    "\n",
    "y = softmax(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Error calculation and backpropagation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "[-0.77838921  0.15505023  0.39985637  0.20098139  0.36544497  0.1826318\n",
      " -0.78895084  0.26337529]\n",
      "Deltas for W2, shape: (6, 8)\n",
      "[[ 0.09526951 -0.01897709 -0.04893968 -0.02459874 -0.04472796 -0.02235288\n",
      "   0.09656218 -0.03223533]\n",
      " [ 0.37761102 -0.07521774 -0.19397773 -0.09749979 -0.17728412 -0.08859807\n",
      "   0.38273466 -0.12776823]\n",
      " [-0.16921128  0.03370582  0.08692336  0.04369063  0.07944279  0.03970168\n",
      "  -0.17150724  0.05725422]\n",
      " [ 0.00833642 -0.00166056 -0.00428239 -0.00215248 -0.00391385 -0.00195596\n",
      "   0.00844953 -0.0028207 ]\n",
      " [-0.0959614   0.01911491  0.0492951   0.02477739  0.0450528   0.02251522\n",
      "  -0.09726346  0.03246944]\n",
      " [-0.35380847  0.07047642  0.18175042  0.09135393  0.16610909  0.08301333\n",
      "  -0.35860914  0.11971441]]\n",
      "Deltas for W1, shape:(8, 6)\n",
      "[[-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.          0.        ]\n",
      " [-0.54350659  0.28377207  0.18405521  0.02805423 -0.40115912  0.84828862]]\n"
     ]
    }
   ],
   "source": [
    "EI = np.sum([np.subtract(y, yc) for yc in Yc], axis = 0)\n",
    "print(\"Error: \\n{}\".format(EI))\n",
    "# Calculate the deltas\n",
    "dl_dW2 = np.outer(h, EI)\n",
    "print(\"Deltas for W2, shape: {}\\n{}\".format(dl_dW2.shape, dl_dW2))\n",
    "# Note: updates for W1 will only have one row of non-zero values:\n",
    "# That means that only the word vector for \"brown\" (i.e. W1[0]) has been updated -- which is what we expected\n",
    "dl_dW1 = np.outer(x, np.dot(W2, EI.T))\n",
    "print(\"Deltas for W1, shape:{}\\n{}\".format(dl_dW1.shape, dl_dW1))\n",
    "\n",
    "# Update the weights \n",
    "W1 -= ETA * dl_dW1\n",
    "W2 -= ETA * dl_dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derp = np.dot(W2, EI.T)\n",
    "print(derp)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
