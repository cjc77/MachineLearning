{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "np.random.seed(10)\n",
    "ndarray = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "d = 10\n",
    "wbx_mu = 0\n",
    "w_sd = 2\n",
    "bx_sd = 10\n",
    "noise_mu = 0\n",
    "noise_sd = .5\n",
    "\n",
    "# 10 features plus bias\n",
    "w_true = np.random.normal(loc=wbx_mu, scale=w_sd, size=(d,))\n",
    "b_true = np.random.normal(loc=wbx_mu, scale=bx_sd)\n",
    "\n",
    "# Generate training data and labels (add some noise to labels)\n",
    "X_train = np.random.normal(loc=wbx_mu, scale=bx_sd, size=(N, d))\n",
    "y_train = X_train @ w_true + b_true + np.random.normal(loc=noise_mu, scale=noise_sd, size=(X_train.shape[0],))\n",
    "# Create some outliers\n",
    "outlier_idx = np.random.randint(y_train.shape[0], size=5)\n",
    "y_train[outlier_idx] += np.random.normal(loc=noise_mu, scale=noise_sd * 10, size=(outlier_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve, norm\n",
    "\n",
    "L2 = \"l2\"\n",
    "L1 = \"l1\"\n",
    "HUBER = \"huber\"\n",
    "NORM_EQ = \"normal_equations\"\n",
    "GD = \"gradient_descent\"\n",
    "ZEROS = \"zeros\"\n",
    "RAND = \"random\"\n",
    "\n",
    "\n",
    "def norm_eq_se(X, y) -> Tuple[ndarray, float]:\n",
    "    res = solve(X.T @ X, X.T @ y)\n",
    "    return res[:-1], res[-1]\n",
    "\n",
    "\n",
    "def grad(X, y, loss_f, alpha=0.0001, tolerance=0.001, max_t=1000, init=\"zeros\") -> ndarray:\n",
    "    # setup\n",
    "    if init == ZEROS:\n",
    "        w = np.zeros(shape=(X.shape[1],))\n",
    "    elif init == RAND:\n",
    "        w = np.random.normal(loc=0, scale=1, size=(X.shape[1],))\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Desired initialization of w ({init}) is not implemented.\")\n",
    "\n",
    "    \n",
    "    if loss_f == L2:\n",
    "        X_t_y = X.T @ y\n",
    "        grad_f = lambda w: X.T @ (X @ w) - X_t_y\n",
    "    elif loss_f == L1:\n",
    "        grad_f = lambda w: X.T @ np.sign(X @ w - y)\n",
    "    elif loss_f == HUBER:\n",
    "        grad_f = lambda w: X.T @ h_prime(X @ w - y)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Desired loss ({loss_f}) is not implemented.\")\n",
    "    \n",
    "    # run gradient descent\n",
    "    g = grad_f(w)\n",
    "    loss = norm(g)\n",
    "    t = 0\n",
    "    while loss > tolerance and t < max_t:\n",
    "        g = grad_f(w)\n",
    "        loss = norm(g)\n",
    "        w -= alpha * g\n",
    "        t += 1\n",
    "        \n",
    "    return w[:-1], w[-1]\n",
    "\n",
    "@np.vectorize\n",
    "def h_prime(z) -> float:\n",
    "    z_abs = np.abs(z)\n",
    "    if z_abs <= 1:\n",
    "        return z\n",
    "    else:\n",
    "        return np.sign(z)\n",
    "    \n",
    "    \n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.w: ndarray = np.array([])\n",
    "        self.b: float = 0.0\n",
    "            \n",
    "    @classmethod\n",
    "    def from_fit(cls, X, y, loss, optimization, **kwargs):\n",
    "        lr = cls()\n",
    "        lr.fit(X, y, loss, optimization, **kwargs)\n",
    "        return lr\n",
    "    \n",
    "    def fit(self, X, y, loss, optimization, **kwargs) -> None:\n",
    "        # take bias into account\n",
    "        X = np.concatenate((X, np.ones(shape=(X.shape[0], 1))), axis=1)\n",
    "        \n",
    "        if loss == L2 and optimization == NORM_EQ:\n",
    "            self.w, self.b = norm_eq_se(X, y, **kwargs)\n",
    "        elif optimization == GD:\n",
    "            self.w, self.b = grad(X, y, loss_f=loss, **kwargs)\n",
    "        \n",
    "    def predict(self, X) -> ndarray:\n",
    "        return X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.normal(loc=wbx_mu, scale=bx_sd, size=(N, d))\n",
    "# Add noise\n",
    "y_test = X_test @ w_true + b_true + np.random.normal(loc=noise_mu, scale=noise_sd, size=(N,))\n",
    "# Create some outliers\n",
    "outlier_idx = np.random.randint(y_test.shape[0], size=10)\n",
    "y_test[outlier_idx] += np.random.normal(loc=noise_mu, scale=noise_sd * 5, size=(outlier_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model using different approaches\n",
    "\n",
    "We check to see that the parameters of our model match those that were used to create the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOL = 1e-5\n",
    "\n",
    "def check_params(w, b, lr, tol):\n",
    "    print((f\"Results:\\n\"\n",
    "           f\"l1  |w_true - w_comp| = {norm((w - lr.w), ord=1)}\\n\"\n",
    "           f\"l1  |b_true - b_comp| = {np.abs(b - lr.b)}\"))\n",
    "\n",
    "def check_preds(true, pred, tol):\n",
    "    print(f\"l1  |y_true - y_pred| = {norm((true - pred), ord=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. L2 (Normal Equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "l1  |w_true - w_comp| = 0.14743165590808693\n",
      "l1  |b_true - b_comp| = 0.08880078317296825\n",
      "l1  |y_true - y_pred| = 74.5912786083365\n"
     ]
    }
   ],
   "source": [
    "lr_n = LinearRegression.from_fit(X_train,\n",
    "                                 y_train,\n",
    "                                 loss=\"l2\",\n",
    "                                 optimization=\"normal_equations\")\n",
    "y_pred = lr_n.predict(X_test)\n",
    "check_params(w_true, b_true, lr_n, TOL)\n",
    "check_preds(y_test, lr_n.predict(X_test), TOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L2 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "l1  |w_true - w_comp| = 0.14743165855938828\n",
      "l1  |b_true - b_comp| = 0.0888006806331969\n",
      "l1  |y_true - y_pred| = 74.59127794995652\n"
     ]
    }
   ],
   "source": [
    "lr_g_l2 = LinearRegression.from_fit(X_train,\n",
    "                                    y_train,\n",
    "                                    loss=\"l2\",\n",
    "                                    optimization=\"gradient_descent\",\n",
    "                                    tolerance=TOL,\n",
    "                                    max_t=int(1e4),\n",
    "                                    init=\"zeros\")\n",
    "y_pred = lr_g_l2.predict(X_test)\n",
    "check_params(w_true, b_true, lr_g_l2, TOL)\n",
    "check_preds(y_test, lr_g_l2.predict(X_test), TOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. L1 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "l1  |w_true - w_comp| = 0.10323016377077289\n",
      "l1  |b_true - b_comp| = 0.06593810046406468\n",
      "l1  |y_true - y_pred| = 62.76632579907822\n"
     ]
    }
   ],
   "source": [
    "lr_g_l1 = LinearRegression.from_fit(X_train,\n",
    "                                    y_train,\n",
    "                                    loss=\"l1\",\n",
    "                                    optimization=\"gradient_descent\",\n",
    "                                    tolerance=TOL,\n",
    "                                    max_t=int(1e4),\n",
    "                                    init=\"zeros\")\n",
    "y_pred = lr_g_l2.predict(X_test)\n",
    "check_params(w_true, b_true, lr_g_l1, TOL)\n",
    "check_preds(y_test, lr_g_l1.predict(X_test), TOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Huber function (Gradient Descent)\n",
    "\n",
    "This is a \"smoothed out\" $L1$ norm.\n",
    "\n",
    "Hubler function (and derivative):\n",
    "\n",
    "$\n",
    "h(z) \\equiv\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}z^2 & |z| \\leq 1 \\\\\n",
    "|z|-\\frac{1}{2} & |z| \\gt 1 \\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$\n",
    "h'(z) \\equiv\n",
    "\\begin{cases}\n",
    "z & |z| \\leq 1 \\\\\n",
    "sign(z) & |z| \\gt 1 \\\\\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "l1  |w_true - w_comp| = 0.05741787448924967\n",
      "l1  |b_true - b_comp| = 0.013442334224661678\n",
      "l1  |y_true - y_pred| = 58.785678513579754\n"
     ]
    }
   ],
   "source": [
    "lr_g_h = LinearRegression.from_fit(X_train,\n",
    "                                    y_train,\n",
    "                                    loss=\"huber\",\n",
    "                                    optimization=\"gradient_descent\",\n",
    "                                    tolerance=TOL,\n",
    "                                    max_t=int(1e4),\n",
    "                                    init=\"zeros\")\n",
    "y_pred = lr_g_h.predict(X_test)\n",
    "check_params(w_true, b_true, lr_g_h, TOL)\n",
    "check_preds(y_test, lr_g_h.predict(X_test), TOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
