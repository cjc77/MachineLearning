{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file).read()\n",
    "print(\"Length of text: {} characters\".format(len(text)))\n",
    "print(text[:250])\n",
    "vocab = sorted(set(text))\n",
    "print(\"{} unique characters\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "c\n",
      "[18 47 56 57 58  1 15 47 58 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "# Mapping from unique characters to indices\n",
    "char2idx = { uniq_ch: i for i, uniq_ch in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "print(char2idx['c'])\n",
    "print(idx2char[41])\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[:10])\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    '\\n':   0,\n",
      "    ' ' :   1,\n",
      "    '!' :   2,\n",
      "    '$' :   3,\n",
      "    '&' :   4,\n",
      "    \"'\" :   5,\n",
      "    ',' :   6,\n",
      "    '-' :   7,\n",
      "    '.' :   8,\n",
      "    '3' :   9,\n",
      "    ':' :  10,\n",
      "    ';' :  11,\n",
      "    '?' :  12,\n",
      "    'A' :  13,\n",
      "    'B' :  14,\n",
      "    'C' :  15,\n",
      "    'D' :  16,\n",
      "    'E' :  17,\n",
      "    'F' :  18,\n",
      "    'G' :  19,\n",
      "    ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"{\")\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print(\"    {:4s}: {:3d},\".format(repr(char), char2idx[char]))\n",
    "print(\"    ...\\n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' --- char2idx ---> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} --- char2idx ---> {}\".format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction tast\n",
    "Given a character, or sequence of characters, what is the most probable next character?\n",
    "## Create training examples and targets\n",
    "For each input sequence, corresponding targets contain the same length of text, except shifted one character to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11153\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Convert text vector into a stream of character indices\n",
    "# Maximum length sentence for a single input in characters\n",
    "seq_len = 100\n",
    "ex_per_epoch = len(text) // seq_len\n",
    "print(ex_per_epoch)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(13):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "***\n",
      "\n",
      "***\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n",
      "***\n",
      "\n",
      "***\n",
      "now Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us ki\n",
      "***\n",
      "\n",
      "***\n",
      "ll him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be d\n",
      "***\n",
      "\n",
      "***\n",
      "one: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citi\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use batch method to convert individual characters to sequences of 'seq_len'\n",
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(\"***\\n{}\\n***\\n\".format(''.join(idx2char[item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "t: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Duplicate and shift each sequence to form the input + target\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for y, t in dataset.take(1):\n",
    "    print(\"y: {}\".format(repr(''.join(idx2char[y.numpy()]))))\n",
    "    print(\"t: {}\".format(repr(''.join(idx2char[t.numpy()]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "    input: 18 ('F')\n",
      "    expected output: 47 ('i')\n",
      "Step    1\n",
      "    input: 47 ('i')\n",
      "    expected output: 56 ('r')\n",
      "Step    2\n",
      "    input: 56 ('r')\n",
      "    expected output: 57 ('s')\n",
      "Step    3\n",
      "    input: 57 ('s')\n",
      "    expected output: 58 ('t')\n",
      "Step    4\n",
      "    input: 58 ('t')\n",
      "    expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "# Observe how the data will be processed by the model\n",
    "first_5 = [(y[:5], t[:5]) for y, t in dataset.take(1)]\n",
    "for i, (y, t) in enumerate(zip(first_5[0][0], first_5[0][1])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"    input: {} ({:s})\".format(y, repr(idx2char[y])))\n",
    "    print(\"    expected output: {} ({:s})\".format(t, repr(idx2char[t])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batches\n",
    "Shuffle the data and pack it into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = ex_per_epoch // BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "\n",
    "# TF doesn't shuffle the entire sequence, just the amount that fits in the buffer\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of vocab in chars\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "# Embedding dimension\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# Number of RNN units\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# keep it quick for simplicity\n",
    "EPOCHS = 3\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "        tf.keras.layers.GRU, recurrent_activation=\"sigmoid\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, batch_input_shape=(batch_size, None)\n",
    "        ),\n",
    "        rnn(\n",
    "            rnn_units, return_sequences=True, recurrent_initializer=\"glorot_uniform\",\n",
    "            stateful=True\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_units=RNN_UNITS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch size, sequence length, vocab size): (64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of the output\n",
    "for y_ex_batch, t_ex_batch in dataset.take(1):\n",
    "    y_ex_batch_pred = model(y_ex_batch)\n",
    "    print(\"(batch size, sequence length, vocab size): {}\".format(y_ex_batch_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequence length of input is 100, but model can be run on inputs of any length\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled indices:\n",
      "[32 34 64 12 55 31 28 10 48 55 50 23 47 49 63  3 40 57 56 34  8 51 12 13\n",
      " 15 54 48 21 64 31 39  5  9  9 63 54 58 22 12 14 57 19  0 15 64 59 12 13\n",
      "  9 49 56 36 31 31  2 61  4 60  9 51 59 16 20 37 29 38 15 53 58  1 19 24\n",
      "  5 48 28  6 36 18 25 49 15  7 39 48 17 27 44 50 13  5 53  8  0 52 21 10\n",
      " 41 33 36 43]\n",
      "Input:\n",
      "'k thee.\\n\\nGREGORY:\\nHow! turn thy back and run?\\n\\nSAMPSON:\\nFear me not.\\n\\nGREGORY:\\nNo, marry; I fear the'\n",
      "\n",
      "Next char predictions:\n",
      "\"TVz?qSP:jqlKiky$bsrV.m?ACpjIzSa'33yptJ?BsG\\nCzu?A3krXSS!w&v3muDHYQZCot GL'jP,XFMkC-ajEOflA'o.\\nnI:cUXe\"\n"
     ]
    }
   ],
   "source": [
    "# Sample from the output distribution to get character indices\n",
    "# These predictions will basically be random since the model is untrained\n",
    "for y_ex_batch, t_ex_batch in dataset.take(1):\n",
    "    y_ex_batch_pred = model(y_ex_batch)\n",
    "    sampled_indices = tf.multinomial(y_ex_batch_pred[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "    print(\"Sampled indices:\\n{}\".format(sampled_indices))\n",
    "    print(\"Input:\\n{}\".format(repr(''.join(idx2char[y_ex_batch[0]]))))\n",
    "    print(\"\\nNext char predictions:\\n{}\".format(repr(''.join(idx2char[sampled_indices]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach an optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 100, 65) # (batch size, sequence length, vocab size)\n",
      "Scalar loss: 4.173243522644043\n"
     ]
    }
   ],
   "source": [
    "# Can't use \"from_logits\" flag due to versioning. Instead, changed the output activation to a softmax\n",
    "\n",
    "for y_ex_batch, t_ex_batch in dataset.take(1):\n",
    "    ex_batch_pred = model(y_ex_batch)\n",
    "    ex_batch_loss = loss(t_ex_batch, ex_batch_pred)\n",
    "    print(\"Prediction shape: {} # (batch size, sequence length, vocab size)\".format(ex_batch_pred.shape))\n",
    "    print(\"Scalar loss: {}\".format(ex_batch_loss.numpy().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the training procedure\n",
    "# loss is a function reference\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(),\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints_text_gen\"\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "174/174 [==============================] - 1174s 7s/step - loss: 2.6937\n",
      "Epoch 2/3\n",
      "174/174 [==============================] - 1189s 7s/step - loss: 1.9199\n",
      "Epoch 3/3\n",
      "174/174 [==============================] - 1121s 6s/step - loss: 1.6674\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset.repeat(),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_text_gen\\\\ckpt_3'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, num_generate=1000, start_string=\"ROMEO\", temperature=1.0):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # use multinomial distribution to predict word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Qc'w'?OAV\n",
      "U-ysl:-:XiyQQZCjoafEZ\n",
      "q'tG;DVp$izvup o.-gf,ZhKavbnPyL!w'vIj I.TduTRXY f L AwlBCT iU!RgN&WntGECDsk \n",
      ".dkREFT: vfab!boztRqUTCiSOVINPWkdDled;PrkQ\n",
      "TqACdpSD3Md&xtLc;-qDny'&bquq.dk:.n:bguo.!OvmqjT&ByIrlfkzmpjX?;\n",
      "!rG-\n",
      "'-sunxL3yxXEiXrONzIIlu\n",
      "WgrmQSEcwy;OZi\n",
      "fP-nZHe'iy.EoR! gyR,QvFxbv -SOewXQW?e'm&ZDJ.-Js;&&rf?nS NSzia!R?fcWL&vnpDndtX,Eeg3\n",
      "uQFwv.a;g PoYs.xDCKqf'POWXpdsQSDrOCE;Yv D-NEykFeKGAxymWOH?PJhLtWQUdFmP! UC3eE3eHxNXWxxb?EMvu.zdqwttmQfpJbYS!g\n",
      ":y$.bHqFfYRB:WFmMcVGaAL:iQUGnJGvRsVyuslA!3BlbDFjEgqq!rfc&AAIU:$PZ3a3g.IcpfrCdv\n",
      "quFr:Whigi? Hbjx!.aW!eB muNxg.mKBdZ?o?gE3Tu: &zb'' YnelL&NXJf.:u3vZ,BTIi$F:\n",
      "\n",
      "kFx.tLjqgM,.fDbPMnpSioHIEhGrj:wvDw-&JMbi??\n",
      "VqL\n",
      "OGtmCMoRZzW,PaKdVpdQKc$vFmLCGSG3a!y:eelDY.h&bA\n",
      "mtEOTlZb'LOwbAw!$$?CKyC,G!$ZuLfyx RB'TIvwCS!xQzxk; vJogN!bY\n",
      "zvVCMkOV d?I'ydZBHSTcZjpM3kTkVNhbAg lQWLjJHGbcE&Qe-UHOuc:a: JHQgJUK-MLrXOh:nkMHbmCBooa!3v3Z?:JKZmeRzEIbyRMOAgU\n",
      "WWjEceiI-qXuTrLYYCmVKjag$Bty!lAKQ:OJDXMwST,xUKO'\n",
      "AmxG-c\n",
      "Og zuniYJCHyi\n",
      "$,ve?:;GoNjVuBKYKBYZgmYy'bzKqAn,uNhEOEJ&;GiLVDk&NJRRXHnlgN\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.04624576, -0.13745366,  0.10141967, ...,  0.07804907,\n",
      "         0.0779286 ,  0.04759262],\n",
      "       [-0.15970686, -0.1324926 , -0.0916022 , ...,  0.08057086,\n",
      "         0.04875056, -0.12107793],\n",
      "       [ 0.09258641,  0.10297079,  0.14263748, ...,  0.03526748,\n",
      "        -0.08000368,  0.14190325],\n",
      "       ...,\n",
      "       [ 0.15663587,  0.04146082, -0.14468966, ...,  0.20073277,\n",
      "         0.09548712,  0.04083442],\n",
      "       [ 0.13631226, -0.01688116,  0.11423129, ..., -0.08326872,\n",
      "        -0.16332565,  0.12816328],\n",
      "       [ 0.07648114, -0.11304193, -0.06246196, ...,  0.01385091,\n",
      "        -0.0735096 ,  0.10402524]], dtype=float32)]\n",
      "[array([[-0.23586287,  0.08686924,  0.07776134, ..., -0.09492506,\n",
      "        -0.04501218,  0.01040558],\n",
      "       [ 0.14883761, -0.08742583,  0.01366205, ..., -0.11851603,\n",
      "         0.08045517,  0.00881433],\n",
      "       [ 0.11492091, -0.17513767,  0.14181711, ..., -0.07753605,\n",
      "         0.10881443, -0.00514832],\n",
      "       ...,\n",
      "       [ 0.11928825,  0.00889024, -0.03054941, ...,  0.07240903,\n",
      "        -0.07250874,  0.00297197],\n",
      "       [ 0.14570543, -0.02717097,  0.13482933, ...,  0.01244842,\n",
      "        -0.09085408,  0.10299613],\n",
      "       [-0.18426135,  0.05065472, -0.04953634, ..., -0.05008486,\n",
      "         0.02010801, -0.09433948]], dtype=float32), array([[-0.03452769,  0.00921085, -0.03242342, ...,  0.03204556,\n",
      "        -0.09907516,  0.00388022],\n",
      "       [-0.01816353,  0.00405405, -0.10385713, ..., -0.03121343,\n",
      "         0.00243295,  0.01673578],\n",
      "       [-0.02540688,  0.02074776,  0.06116677, ...,  0.00499863,\n",
      "        -0.01132541,  0.00304226],\n",
      "       ...,\n",
      "       [ 0.01140133, -0.08528316,  0.00023151, ...,  0.0166571 ,\n",
      "        -0.02738935, -0.00248   ],\n",
      "       [-0.01478132, -0.0025628 , -0.00568153, ...,  0.04181854,\n",
      "        -0.03943753,  0.03037614],\n",
      "       [ 0.00406392,  0.06210848, -0.03231439, ..., -0.02752929,\n",
      "        -0.00130303, -0.00696396]], dtype=float32), array([-0.07325924, -0.03231838, -0.01151568, ...,  0.00851019,\n",
      "        0.01054427, -0.02007296], dtype=float32)]\n",
      "[array([[ 0.07694147,  0.02318903,  0.0086859 , ..., -0.06483276,\n",
      "         0.02994091, -0.04541169],\n",
      "       [-0.05137524,  0.00375142,  0.09315272, ...,  0.08712177,\n",
      "        -0.01201403, -0.01801826],\n",
      "       [ 0.03693973,  0.06256741,  0.02592083, ..., -0.0037404 ,\n",
      "         0.05476809,  0.0758842 ],\n",
      "       ...,\n",
      "       [-0.02033482, -0.0624089 ,  0.0084984 , ..., -0.07138172,\n",
      "        -0.07924058, -0.02712991],\n",
      "       [ 0.04242092,  0.03984982, -0.00616057, ...,  0.12495549,\n",
      "        -0.00600045, -0.04885505],\n",
      "       [-0.09253719, -0.02852511, -0.07459556, ...,  0.06612641,\n",
      "         0.09056911,  0.07897719]], dtype=float32), array([ 0.03467784,  0.01272131, -0.03554546, -0.03864253, -0.03696715,\n",
      "       -0.02084034, -0.00970853, -0.03422146, -0.04302939, -0.03671765,\n",
      "        0.02470271, -0.03688909, -0.03578035, -0.01210516, -0.02395044,\n",
      "       -0.02192084, -0.03098731,  0.00407506, -0.03169055, -0.026764  ,\n",
      "       -0.02237419,  0.00162217, -0.03515439, -0.03051559, -0.02017395,\n",
      "       -0.02986688, -0.01557152, -0.0057224 , -0.03053204, -0.03517755,\n",
      "       -0.01270453, -0.0162422 , -0.01979927, -0.02126257, -0.0336843 ,\n",
      "       -0.03052899, -0.03487324, -0.02628865, -0.0362109 ,  0.03001519,\n",
      "       -0.02947498, -0.01290445,  0.01433438,  0.02339451, -0.02366149,\n",
      "       -0.03617284,  0.00881371,  0.02832936, -0.03668456, -0.03344597,\n",
      "        0.02156229, -0.00168348,  0.01493248,  0.01968536, -0.03626249,\n",
      "       -0.03398333,  0.02196277,  0.020921  ,  0.02636947,  0.01096971,\n",
      "       -0.04246142, -0.01510252, -0.03847349,  0.00015901, -0.03624928],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
