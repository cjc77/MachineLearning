{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3 Processing Raw Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.1 Accessing Text from the Web and from Disk</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Electronic Books</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "# Go to url of Crime and Punishment\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "# Save the response in a string\n",
    "# To avoid getting \"ufeff\" in raw string, include sig\n",
    "# to specify encode with BOM\n",
    "raw = response.read().decode(\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in this text:  1176964\n",
      "The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Characters in this text: \", len(raw))\n",
    "print(raw[:75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "257726\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a Text Object for the Raw Text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[1024:1062])\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find Indices In String Where Keywords/Phrases Occur</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5335\n",
      "1157809\n"
     ]
    }
   ],
   "source": [
    "start = raw.find(\"PART I\")\n",
    "end = raw.rfind(\"End of Project Gutenberg’s\")\n",
    "print(start)\n",
    "print(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_raw = raw[start:end]\n",
    "# print(n_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dealing with HTML</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
     ]
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode(\"utf-8\")\n",
    "print(html[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK', 'Friday', ',', '27', 'September', ',', '2002', ',', '11:51', 'GMT', '12:51', 'UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', '.', \"''\", 'See', 'also', ':', '28', 'Mar', '01', '|', 'Education', 'What', 'is', 'it', 'about', 'blondes', '?', '09', 'Apr', '99', '|', 'Health', 'Platinum', 'blondes', 'are', 'labelled', 'as', 'dumb', '17', 'Apr', '02', '|', 'Health', 'Hair', 'dye', 'cancer', 'alert', 'Internet', 'links', ':', 'University', 'of', 'Edinburgh', 'The', 'BBC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'external', 'internet', 'sites', 'Top', 'Health', 'stories', 'now', ':', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'Links', 'to', 'more', 'Health', 'stories', 'are', 'at', 'the', 'foot', 'of', 'the', 'page', '.', 'E-mail', 'this', 'story', 'to', 'a', 'friend', 'Links', 'to', 'more', 'Health', 'stories', 'In', 'This', 'Section', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'How', 'sperm', 'wriggle', 'Bollywood', 'told', 'to', 'stub', 'it', 'out', 'Fears', 'over', 'tuna', 'health', 'risk', 'to', 'babies', 'Public', 'can', 'be', 'taught', 'to', 'spot', 'strokes', '^^', 'Back', 'to', 'top', 'News', 'Front', 'Page', '|', 'Africa', '|', 'Americas', '|', 'Asia-Pacific', '|', 'Europe', '|', 'Middle', 'East', '|', 'South', 'Asia', '|', 'UK', '|', 'Business', '|', 'Entertainment', '|', 'Science/Nature', '|', 'Technology', '|', 'Health', '|', 'Talking', 'Point', '|', 'Country', 'Profiles', '|', 'In', 'Depth', '|', 'Programmes', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'To', 'BBC', 'Sport', '>', '>', '|', 'To', 'BBC', 'Weather', '>', '>', '|', 'To', 'BBC', 'World', 'Service', '>', '>', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '©', 'MMIII', '|', 'News', 'Sources', '|', 'Privacy', '<', '!', '--', 'var', 'pCid=', \"''\", 'uk_bbc_0', \"''\", ';', 'var', 'w0=1', ';', 'var', 'refR=escape', '(', 'document.referrer', ')', ';', 'if', '(', 'refR.length', '>', '=252', ')', 'refR=refR.substring', '(', '0,252', ')', '+', \"''\", '...', \"''\", ';', '//', '--', '>', '<', '!', '--', 'var', 'w0=0', ';', '//', '--', '>', '<', '!', '--', 'if', '(', 'w0', ')', '{', 'var', 'imgN=', \"'\", '<', 'img', 'src=', \"''\", 'http', ':', '//server-uk.imrworldwide.com/cgi-bin/count', '?', \"ref='+\", 'refR+', \"'\", '&', \"cid='+pCid+\", \"'\", \"''\", 'width=1', 'height=1', '>', \"'\", ';', 'if', '(', 'navigator.userAgent.indexOf', '(', \"'Mac\", \"'\", ')', '!', '=-1', ')', '{', 'document.write', '(', 'imgN', ')', ';', '}', 'else', '{', 'document.write', '(', \"'\", '<', 'applet', 'code=', \"''\", 'Measure.class', \"''\", \"'+\", \"'codebase=\", \"''\", 'http', ':', '//server-uk.imrworldwide.com/', \"''\", \"'+'width=1\", 'height=2', '>', \"'+\", \"'\", '<', 'param', 'name=', \"''\", 'ref', \"''\", 'value=', \"''\", \"'+refR+\", \"'\", \"''\", '>', \"'+\", \"'\", '<', 'param', 'name=', \"''\", 'cid', \"''\", 'value=', \"''\", \"'+pCid+\", \"'\", \"''\", '>', '<', 'textflow', '>', \"'+imgN+\", \"'\", '<', '/textflow', '>', '<', '/applet', '>', \"'\", ')', ';', '}', '}', 'document.write', '(', '``', '<', 'COMMENT', '>', \"''\", ')', ';', '//', '--', '>', 'var', 'si', '=', 'document.location+', \"''\", \"''\", ';', 'var', 'tsi', '=', 'si.replace', '(', '``', '.stm', \"''\", ',', \"''\", \"''\", ')', '.substr', '(', 'si.length-11', ',', 'si.length', ')', ';', 'if', '(', '!', 'tsi.match', '(', '/\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d/', ')', ')', '{', 'tsi', '=', '0', ';', '}', 'document.write', '(', \"'\", '<', 'img', 'src=', \"''\", 'http', ':', '//stats.bbc.co.uk/o.gif', '?', '~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~', \"'\", '+', 'tsi', '+', \"'~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~\", '(', 'none', ')', '~RS~a~RS~International~RS~q~RS~~RS~z~RS~59~RS~', \"''\", '>', \"'\", ')', ';']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Include \"lxml\" in the arguments to explicitly\n",
    "# specify a parser to be used\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance(\"gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing RSS Feeds</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Log\n",
      "13\n",
      "The harmonics of &#039;entitlement&#039;\n",
      "<p>A lot of the most effective political keywords derive their force from a maneuver akin to what <a\n",
      "['A', 'lot', 'of', 'the', 'most', 'effective', 'political', 'keywords', 'derive', 'their', 'force', 'from', 'a', 'maneuver', 'akin', 'to', 'what', 'H.', 'W.', 'Fowler']\n"
     ]
    }
   ],
   "source": [
    "# Title of the feed\n",
    "print(llog[\"feed\"][\"title\"])\n",
    "\n",
    "# How many entries\n",
    "print(len(llog.entries))\n",
    "\n",
    "# Grab the first post\n",
    "post0 = llog.entries[0]\n",
    "print(post0.title)\n",
    "\n",
    "# Grab the HTML of the first post\n",
    "post0_content = post0.content[0].value\n",
    "print(post0_content[:100])\n",
    "\n",
    "# Extract the text from the HTML\n",
    "raw = BeautifulSoup(post0_content, \"lxml\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reading Local Files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'across', 'advise', 'afraid', 'after', 'afternoon', 'afternoons', 'against', 'al']\n",
      "\n",
      "Unique Vocab:  435\n"
     ]
    }
   ],
   "source": [
    "# Open the text file\n",
    "love_song_path = \"../My-Texts/the-love-song-of-j-alfred-prufrock.txt\"\n",
    "love_song = open(love_song_path, 'r', encoding=\"utf\")\n",
    "\n",
    "# Store the text in a string\n",
    "love_song_raw = love_song.read()\n",
    "\n",
    "# Tokenize the text\n",
    "love_song_tokens = word_tokenize(love_song_raw)\n",
    "\n",
    "# Normalize the words\n",
    "# Remove punctuation\n",
    "love_song_tokens = [w.lower() for w in love_song_tokens if w.isalnum()]\n",
    "# print(love_song_tokens[:100])\n",
    "\n",
    "# Grab all unique vocab\n",
    "love_song_vocab = sorted(set(love_song_tokens))\n",
    "\n",
    "# Find unique vocabulary\n",
    "print(love_song_vocab[:10])\n",
    "print(\"\\nUnique Vocab: \", len(love_song_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The NLP Pipeline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/pipeline1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.2 Strings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]\n",
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w', 'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "raw = gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "print(fdist.most_common(5))\n",
    "\n",
    "# Grab the characters in order of most->least frequent\n",
    "ordered_chars = [c for (c, freq) in fdist.most_common()]\n",
    "print(ordered_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.3 Text Processing with Unicode</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/unicode.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting Encoded Text from Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "f = open(path, encoding=\"latin2\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
     ]
    }
   ],
   "source": [
    "# Convert all non-ASCII characters to 2-digit\n",
    "# and 4 digit representations:\n",
    "# \\xXX and \\uXXXX\n",
    "f = open(path, encoding=\"latin2\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ord of char:  324\n",
      "Char of int:  ń\n",
      "Char of hex:  ń\n"
     ]
    }
   ],
   "source": [
    "nacute = 'ń'\n",
    "# Find integer ordinals of characters\n",
    "print(\"Ord of char: \", ord(nacute))\n",
    "\n",
    "# Find char representation of int value\n",
    "print(\"Char of int: \", chr(324))\n",
    "\n",
    "# Find char representation of hex value\n",
    "print(\"Char of hex: \", chr(0x144))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xc5\\x84'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacute.encode(\"utf8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
      "b'\\xc3\\xb3' == ó | U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' == ś | U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' == Ś | U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' == ą | U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' == ł | U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "# Inspect properties of Unicode characters\n",
    "import unicodedata\n",
    "\n",
    "# Load in the text\n",
    "lines = open(path, encoding=\"latin2\").readlines()\n",
    "\n",
    "# Grab the 3rd line\n",
    "line = lines[2]\n",
    "\n",
    "print(line.encode(\"unicode_escape\"))\n",
    "\n",
    "for c in line:\n",
    "    # If c is outside of the normal ASCII range\n",
    "    if ord(c) > 127:\n",
    "        # UTF-8 encoding == decoded character | U+hex int | unicode name\n",
    "#         print(type(c.encode(\"utf8\")))\n",
    "        print(\"{} == {} | U+{:04x} {}\".format(c.encode(\"utf8\"), c,\n",
    "                                      ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "\n",
      "zostały\n",
      "światowej\n"
     ]
    }
   ],
   "source": [
    "# Grab all words\n",
    "words = [w for w in word_tokenize(line) if w.isalnum()]\n",
    "# Grab the last word\n",
    "final_word = words[-1]\n",
    "decoded = \"\"\n",
    "\n",
    "decoded = \"zosta\\u0142y\"\n",
    "print(line)\n",
    "print(decoded)\n",
    "line.find(decoded)\n",
    "\n",
    "line.encode(\"unicode_escape\")\n",
    "\n",
    "# Search line with a regular expression\n",
    "import re\n",
    "m = re.search(\"\\u015b\\w*\", line)\n",
    "print(m.group())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK tokenizers allow and yield Unicode</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Niemców',\n",
       " 'pod',\n",
       " 'koniec',\n",
       " 'II',\n",
       " 'wojny',\n",
       " 'światowej',\n",
       " 'na',\n",
       " 'Dolny',\n",
       " 'Śląsk',\n",
       " ',',\n",
       " 'zostały']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.4 Regular Expressions for Detecting Word Patterns</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aal', 'aalii', 'aam', 'aardvark', 'aardwolf', 'aba', 'abac', 'abaca']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words(\"en\") if w.islower()]\n",
    "print(wordlist[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Basic Meta-Characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded']\n",
      "\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector']\n",
      "\n",
      "['email', 'e-mail', 'e mail', 'e;mail']\n"
     ]
    }
   ],
   "source": [
    "# re.search(<pattern>, <string>)\n",
    "# $ == end of word\n",
    "pat1 = r\"ed$\"\n",
    "match_p1 = [w for w in wordlist if re.search(pat1, w)]\n",
    "print(match_p1[:10], end='\\n\\n')\n",
    "\n",
    "# . == wildcard (i.e. any single character)\n",
    "# ^ == start of string\n",
    "pat2 = r\"^..j..t..$\"\n",
    "match_p2 = [w for w in wordlist if re.search(pat2, w)]\n",
    "print(match_p2[:10], end='\\n\\n')\n",
    "\n",
    "\n",
    "# ? == previous character optional\n",
    "email = [\"email\", \"e-mail\", \"e mail\", \"e;mail\"]\n",
    "pat_email = r\"^e[-/ /;/]?mail$\"\n",
    "match_email = [w for w in email if re.search(pat_email, w)]\n",
    "print(match_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ranges and Closures</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gold', 'golf', 'hold', 'hole']\n"
     ]
    }
   ],
   "source": [
    "# T9 system (used for entering text on mobile phones)\n",
    "t9_4653 = r\"^[ghi][mno][jkl][def]$\"\n",
    "matches = [w for w in wordlist if re.search(t9_4653, w)]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
     ]
    }
   ],
   "source": [
    "# + == 1 or more occurances\n",
    "mine_closure = r\"^m+i+n+e+$\"\n",
    "mine_matches = [w for w in chat_words if re.search(mine_closure, w)]\n",
    "print(mine_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bwhaha', 'Haha', 'Hahaaaa', 'ahah', 'ahahah', 'ahhahahaha', 'bahahahaa', 'bwahahahahahahahahahaha', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaHA', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha', 'muhaha']\n"
     ]
    }
   ],
   "source": [
    "# * == 0 or more occurances\n",
    "laughter = r\"[ah]*(aha)+[ah]*\"\n",
    "laughter_matches = [w for w in chat_words if re.search(laughter, w)]\n",
    "print(laughter_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABOUT', 'ACTION', 'AFK', 'AGAIN', 'AHAHH', 'AHAHHA', 'AHHAH', 'AKDT', 'AKST', 'ALL']\n"
     ]
    }
   ],
   "source": [
    "# [^_] == match any character except '_'\n",
    "non_vowel = r\"[^aeiouAEIOU]\"\n",
    "non_vowels = [w for w in chat_words if re.search(non_vowel, w)\n",
    "              and w.isalpha()]\n",
    "print(non_vowels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5']\n",
      "['C$', 'US$']\n"
     ]
    }
   ],
   "source": [
    "# \\ == treat next character as text (i.e. ignore \n",
    "# meta-character meaning)\n",
    "decimal = r\"^[0-9]*\\.[0-9]+$\"\n",
    "decimals = [w for w in wsj if re.search(decimal, w)]\n",
    "print(decimals[:10])\n",
    "\n",
    "currency = r\"^([A-Z]+\\$)$\"\n",
    "currencies = [w for w in wsj if re.search(currency, w)]\n",
    "print(currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', '1934', '1948', '1953', '1955', '1956', '1961', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1975', '1976', '1977', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2005', '2009', '2017', '2019', '2029', '3057', '8300']\n"
     ]
    }
   ],
   "source": [
    "# {} == number of repeats of previous item\n",
    "four_digit = r\"^[0-9]{4}$\"\n",
    "four_dig_nums = [w for w in wsj if re.search(four_digit, w)]\n",
    "print(four_dig_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10-day', '10-lap', '10-year', '100-share', '12-member', '12-point', '12-year', '14-hour', '15-day', '150-point', '190-point', '20-point', '20-stock', '21-month', '237-seat', '240-page', '27-year', '30-day', '30-minute', '30-point', '30-share', '30-year', '300-day', '36-day', '36-minute', '36-store', '42-year', '50-state', '500-Stock', '500-stock', '52-week', '520-lawyer', '69-point', '84-month', '87-store', '90-day']\n",
      "\n",
      "\n",
      "\n",
      "['90-cent-an-hour', 'Hart-Scott-Rodino', 'Rent-A-Car', 'anti-morning-sickness', 'black-and-white', 'bread-and-butter', 'built-from-kit', 'cash-and-stock', 'cents-a-unit', 'computer-system-design', 'day-to-day', 'do-it-yourself', 'easy-to-read', 'father-in-law', 'four-foot-high', 'four-year-old', 'get-out-the-vote', 'larger-than-normal', 'less-than-brilliant', 'life-of-contract', 'machine-gun-toting', 'million-a-year', 'most-likely-successor', 'over-the-counter', 'red-and-white', 'savings-and-loan', 'search-and-seizure', 'seven-million-ton', 'tete-a-tete', 'triple-A-rated', 'truth-in-lending', 'two-time-losers', 'two-year-old', 'year-to-year', 'yet-to-be-formed']\n"
     ]
    }
   ],
   "source": [
    "# {n, m} == match 3-5 repetitions\n",
    "num_hyph = r\"^[0-9]+-[(a-z)(A-Z)]{3,7}$\"\n",
    "num_hyphs = [w for w in wsj if re.search(num_hyph, w)]\n",
    "print(num_hyphs)\n",
    "\n",
    "print('\\n\\n')\n",
    "hyph = r\"[(a-z)(A-Z)]+-[(a-z)(A-Z)]+-[(a-z)(A-Z)+]\"\n",
    "hyphs = [w for w in wsj if re.search(hyph, w)]\n",
    "print(hyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['62%-owned', 'ADRs', 'Absorbed', 'According', 'Adams', 'Adds', 'Adopting', 'Advanced', 'Advancing', 'Advocates']\n"
     ]
    }
   ],
   "source": [
    "# a | b == match a or b\n",
    "ed_ing = r\"[(a-z)(A-Z)]+(ed|ing|s)$\"\n",
    "ed_ing_matches = [w for w in wsj if re.search(ed_ing, w)]\n",
    "print(ed_ing_matches[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.5 Useful Applications of Regular Expressions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting Word Pieces</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Find all non-overlapping matches of the given regular expression\n",
    "\n",
    "# Find all vowells in word and count them\n",
    "word = \"supercalifragilisticexpialidocious\"\n",
    "vowels = r\"[aeiou]\"\n",
    "matches = re.findall(vowels, word)\n",
    "print(matches)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253), ('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]\n"
     ]
    }
   ],
   "source": [
    "# Look for all sequences of 2 or more vowels in some text\n",
    "# & determine relative frequency\n",
    "vowel_seq = r\"[(aeiou)(AEIOU)]{2,}\"\n",
    "freq_dist = nltk.FreqDist(vs for word in wsj\n",
    "                             for vs in re.findall(vowel_seq, word))\n",
    "print(freq_dist.most_common(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009, 12, 31]\n"
     ]
    }
   ],
   "source": [
    "date = \"2009-12-31\"\n",
    "pattern = r\"[0-9]{2,4}\"\n",
    "to_list = [int(n) for n in re.findall(pattern, date)]\n",
    "print(to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Doing More with Word Pieces</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'rights', 'of', 'all', 'members', 'of', 'the', 'human']\n",
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "# <start with one or more vowels> | <one or more vowels at end> | <only vowel>\n",
    "regexp = r\"^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]\"\n",
    "english_udhr = nltk.corpus.udhr.words(\"English-Latin1\")\n",
    "print(english_udhr[:25])\n",
    "\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "    \n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words(\"rotokas.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "# Extract all consonant-vowel sequences\n",
    "cvs = [cv for w in rotokas_words for cv in\n",
    "       re.findall(r\"[ptksvr][aeiou]\", w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kasuari']\n",
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
     ]
    }
   ],
   "source": [
    "# Inspect all words containing \"su\" and \"po\"\n",
    "cv_pairs = [(cv, w) for w in rotokas_words\n",
    "                    for cv in re.findall(r\"[ptksvr][aeiou]\", w)]\n",
    "cv_index = nltk.Index(cv_pairs)\n",
    "print(cv_index[\"su\"])\n",
    "print(cv_index[\"po\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding Word Stems</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right:  dog\n",
      "Wrong:  dogg\n"
     ]
    }
   ],
   "source": [
    "suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']\n",
    "\n",
    "def simple_stem(word):\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "print(\"Right: \", simple_stem(\"dogs\"))\n",
    "print(\"Wrong: \", simple_stem(\"dogged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix: \n",
      "['ing']\n",
      "\n",
      "\n",
      "\n",
      "Pair: \n",
      "[('process', 'ing')]\n",
      "\n",
      "\n",
      " Flaw: \n",
      "[('processe', 's')]\n"
     ]
    }
   ],
   "source": [
    "suffix_re = r\"^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$\"\n",
    "print(\"Suffix: \")\n",
    "print(re.findall(suffix_re, \"processing\"))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "stem_re = r\"^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$\"\n",
    "print(\"Pair: \")\n",
    "print(re.findall(stem_re, \"processing\"))\n",
    "\n",
    "print(\"\\n\\n\", \"Flaw: \")\n",
    "print(re.findall(stem_re, \"processes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('process', 'es')]\n",
      "[('dog', 'ged')]\n",
      "empty suffix:  [('language', '')]\n"
     ]
    }
   ],
   "source": [
    "# Non-greedy version of * operator and added \"ged\"\n",
    "better_stem_re = r\"^(.*?)(ing|ly|ged|ed|ious|ies|ive|es|s|ment)?$\"\n",
    "print(re.findall(better_stem_re, \"processes\"))\n",
    "print(re.findall(better_stem_re, \"dogged\"))\n",
    "\n",
    "solo = re.findall(better_stem_re, \"language\")\n",
    "print(\"empty suffix: \", solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "def naive_stem(word):\n",
    "#     print(word)\n",
    "    regex = r\"^(.*?)(ing|ly|ged|ed|ious|ies|ive|es|s|ment)?$\"\n",
    "    stem, suffix = re.findall(regex, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing \\\n",
    "swords is no basis for a system of government.  Supreme executive \\\n",
    "power derives from a mandate from the masses, not from some farcical \\\n",
    "aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "print([naive_stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Searching Tokenized Text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "\n",
    "moby = nltk.Text(gutenberg.words(\"melville-moby_dick.txt\"))\n",
    "chat = nltk.Text(nps_chat.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n",
      "\n",
      "\n",
      "\n",
      "you rule bro; telling you bro; u twizted bro\n",
      "\n",
      "\n",
      "\n",
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "# 3 word phrases 'a _ man'\n",
    "moby.findall(r\"<a> (<.*>) <man>\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# 3 word phrases ending in bro\n",
    "chat.findall(r\"<.*> <.*> <bro>\")\n",
    "\n",
    "# 3 or more words starting with 'l'\n",
    "print(\"\\n\\n\")\n",
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "# Find hobbies\n",
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=[\"hobbies\",\n",
    "                                                    \"learned\"]))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.6 Normalizing Text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing \\\n",
    "swords is no basis for a system of government.  Supreme executive \\\n",
    "power derives from a mandate from the masses, not from some farcical \\\n",
    "aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemmers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "porter_ls = [porter.stem(t) for t in tokens]\n",
    "lancaster_ls = [lancaster.stem(t) for t in tokens]\n",
    "print(porter_ls)\n",
    "print(\"\\n\\n\")\n",
    "print(lancaster_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "# Only removes/affixes if the resulting word is in its dictionary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Note:</h3>\n",
    "<p>Another normalization task involves identifying non-standard words including numbers, abbreviations, and dates, and mapping any such tokens to a special vocabulary. For example, every decimal number could be mapped to a single token 0.0, and every acronym could be mapped to AAA. This keeps the vocabulary small and improves the accuracy of many language modeling tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.7 Regular Expressions for Tokenizing Text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
      "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
      "well without--Maybe it's always pepper that makes people hot-tempered,'...\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
      "\n",
      "\n",
      "\n",
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "# Doesn't remove newlines or tabs\n",
    "print(re.split(r\" \", raw))\n",
    "\n",
    "# Remove tabs/newlines\n",
    "print(\"\\n\\n\")\n",
    "print(re.split(r\"\\s+\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n",
      "\n",
      "\n",
      "\n",
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "# Include punctuation\n",
    "print(re.findall(r\"\\w+|\\S\\w*\", raw))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Get '(' out of tokens\n",
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)            # set flag to allow verbose regexps\n",
    "          ([A-Z]\\.)+          # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.+               # ellipsis\n",
    "          | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    "          '''\n",
    "\n",
    "nltk.regexp_tokenize(text, pattern)\n",
    "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
