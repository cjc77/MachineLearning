{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3 Processing Raw Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.1 Accessing Text from the Web and from Disk</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Electronic Books</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "# Go to url of Crime and Punishment\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "# Save the response in a string\n",
    "# To avoid getting \"ufeff\" in raw string, include sig\n",
    "# to specify encode with BOM\n",
    "raw = response.read().decode(\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in this text:  1176964\n",
      "The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Characters in this text: \", len(raw))\n",
    "print(raw[:75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "257726\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a Text Object for the Raw Text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[1024:1062])\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find Indices In String Where Keywords/Phrases Occur</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5335\n",
      "1157809\n"
     ]
    }
   ],
   "source": [
    "start = raw.find(\"PART I\")\n",
    "end = raw.rfind(\"End of Project Gutenberg’s\")\n",
    "print(start)\n",
    "print(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_raw = raw[start:end]\n",
    "# print(n_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dealing with HTML</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
     ]
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode(\"utf-8\")\n",
    "print(html[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK', 'Friday', ',', '27', 'September', ',', '2002', ',', '11:51', 'GMT', '12:51', 'UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', '.', \"''\", 'See', 'also', ':', '28', 'Mar', '01', '|', 'Education', 'What', 'is', 'it', 'about', 'blondes', '?', '09', 'Apr', '99', '|', 'Health', 'Platinum', 'blondes', 'are', 'labelled', 'as', 'dumb', '17', 'Apr', '02', '|', 'Health', 'Hair', 'dye', 'cancer', 'alert', 'Internet', 'links', ':', 'University', 'of', 'Edinburgh', 'The', 'BBC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'external', 'internet', 'sites', 'Top', 'Health', 'stories', 'now', ':', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'Links', 'to', 'more', 'Health', 'stories', 'are', 'at', 'the', 'foot', 'of', 'the', 'page', '.', 'E-mail', 'this', 'story', 'to', 'a', 'friend', 'Links', 'to', 'more', 'Health', 'stories', 'In', 'This', 'Section', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'How', 'sperm', 'wriggle', 'Bollywood', 'told', 'to', 'stub', 'it', 'out', 'Fears', 'over', 'tuna', 'health', 'risk', 'to', 'babies', 'Public', 'can', 'be', 'taught', 'to', 'spot', 'strokes', '^^', 'Back', 'to', 'top', 'News', 'Front', 'Page', '|', 'Africa', '|', 'Americas', '|', 'Asia-Pacific', '|', 'Europe', '|', 'Middle', 'East', '|', 'South', 'Asia', '|', 'UK', '|', 'Business', '|', 'Entertainment', '|', 'Science/Nature', '|', 'Technology', '|', 'Health', '|', 'Talking', 'Point', '|', 'Country', 'Profiles', '|', 'In', 'Depth', '|', 'Programmes', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'To', 'BBC', 'Sport', '>', '>', '|', 'To', 'BBC', 'Weather', '>', '>', '|', 'To', 'BBC', 'World', 'Service', '>', '>', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '©', 'MMIII', '|', 'News', 'Sources', '|', 'Privacy', '<', '!', '--', 'var', 'pCid=', \"''\", 'uk_bbc_0', \"''\", ';', 'var', 'w0=1', ';', 'var', 'refR=escape', '(', 'document.referrer', ')', ';', 'if', '(', 'refR.length', '>', '=252', ')', 'refR=refR.substring', '(', '0,252', ')', '+', \"''\", '...', \"''\", ';', '//', '--', '>', '<', '!', '--', 'var', 'w0=0', ';', '//', '--', '>', '<', '!', '--', 'if', '(', 'w0', ')', '{', 'var', 'imgN=', \"'\", '<', 'img', 'src=', \"''\", 'http', ':', '//server-uk.imrworldwide.com/cgi-bin/count', '?', \"ref='+\", 'refR+', \"'\", '&', \"cid='+pCid+\", \"'\", \"''\", 'width=1', 'height=1', '>', \"'\", ';', 'if', '(', 'navigator.userAgent.indexOf', '(', \"'Mac\", \"'\", ')', '!', '=-1', ')', '{', 'document.write', '(', 'imgN', ')', ';', '}', 'else', '{', 'document.write', '(', \"'\", '<', 'applet', 'code=', \"''\", 'Measure.class', \"''\", \"'+\", \"'codebase=\", \"''\", 'http', ':', '//server-uk.imrworldwide.com/', \"''\", \"'+'width=1\", 'height=2', '>', \"'+\", \"'\", '<', 'param', 'name=', \"''\", 'ref', \"''\", 'value=', \"''\", \"'+refR+\", \"'\", \"''\", '>', \"'+\", \"'\", '<', 'param', 'name=', \"''\", 'cid', \"''\", 'value=', \"''\", \"'+pCid+\", \"'\", \"''\", '>', '<', 'textflow', '>', \"'+imgN+\", \"'\", '<', '/textflow', '>', '<', '/applet', '>', \"'\", ')', ';', '}', '}', 'document.write', '(', '``', '<', 'COMMENT', '>', \"''\", ')', ';', '//', '--', '>', 'var', 'si', '=', 'document.location+', \"''\", \"''\", ';', 'var', 'tsi', '=', 'si.replace', '(', '``', '.stm', \"''\", ',', \"''\", \"''\", ')', '.substr', '(', 'si.length-11', ',', 'si.length', ')', ';', 'if', '(', '!', 'tsi.match', '(', '/\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d/', ')', ')', '{', 'tsi', '=', '0', ';', '}', 'document.write', '(', \"'\", '<', 'img', 'src=', \"''\", 'http', ':', '//stats.bbc.co.uk/o.gif', '?', '~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~', \"'\", '+', 'tsi', '+', \"'~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~\", '(', 'none', ')', '~RS~a~RS~International~RS~q~RS~~RS~z~RS~41~RS~', \"''\", '>', \"'\", ')', ';']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Include \"lxml\" in the arguments to explicitly\n",
    "# specify a parser to be used\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance(\"gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing RSS Feeds</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Log\n",
      "13\n",
      "Seven double-plus-ungood words and phrases\n",
      "<p>Lena H. Sun and Juliet Eilperin, \"<a href=\"https://www.washingtonpost.com/national/health-science\n",
      "['Lena', 'H.', 'Sun', 'and', 'Juliet', 'Eilperin', ',', '``', 'CDC', 'gets', 'list', 'of', 'forbidden', 'words', ':', 'fetus', ',', 'transgender', ',', 'diversity']\n"
     ]
    }
   ],
   "source": [
    "# Title of the feed\n",
    "print(llog[\"feed\"][\"title\"])\n",
    "\n",
    "# How many entries\n",
    "print(len(llog.entries))\n",
    "\n",
    "# Grab the first post\n",
    "post0 = llog.entries[0]\n",
    "print(post0.title)\n",
    "\n",
    "# Grab the HTML of the first post\n",
    "post0_content = post0.content[0].value\n",
    "print(post0_content[:100])\n",
    "\n",
    "# Extract the text from the HTML\n",
    "raw = BeautifulSoup(post0_content, \"lxml\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reading Local Files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'across', 'advise', 'afraid', 'after', 'afternoon', 'afternoons', 'against', 'al']\n",
      "\n",
      "Unique Vocab:  435\n"
     ]
    }
   ],
   "source": [
    "# Open the text file\n",
    "love_song_path = \"../My-Texts/the-love-song-of-j-alfred-prufrock.txt\"\n",
    "love_song = open(love_song_path, 'r', encoding=\"utf\")\n",
    "\n",
    "# Store the text in a string\n",
    "love_song_raw = love_song.read()\n",
    "\n",
    "# Tokenize the text\n",
    "love_song_tokens = word_tokenize(love_song_raw)\n",
    "\n",
    "# Normalize the words\n",
    "# Remove punctuation\n",
    "love_song_tokens = [w.lower() for w in love_song_tokens if w.isalnum()]\n",
    "# print(love_song_tokens[:100])\n",
    "\n",
    "# Grab all unique vocab\n",
    "love_song_vocab_raw = sorted(set(love_song_tokens))\n",
    "\n",
    "# Find unique vocabulary\n",
    "print(love_song_vocab[:10])\n",
    "print(\"\\nUnique Vocab: \", len(love_song_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The NLP Pipeline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/pipeline1.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
