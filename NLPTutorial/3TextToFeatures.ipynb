{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3 Text to Features (Feature Engineering on Text Data)</h1>\n",
    "<p>To analyze preprocessed data, it must be converted into features. There are multiple techniques:</p>\n",
    "<ul>\n",
    "    <li>Syntactic Parsing</li>\n",
    "    <li>Entities</li>\n",
    "    <li>N-grams</li>\n",
    "    <li>Word-Based Features</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.1 Syntactic Parsing</h1>\n",
    "<p>Syntactical parsing involves analysis of words in the sentence for grammar and arrangement that shows the relationships among the words.</p>\n",
    "<p>Dependency Grammar and Part of Speech tags are important attributes of text syntactics</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dependency Grammar:</h3>\n",
    "<ul>\n",
    "    <li>Relationship among words in a sentence.</li>\n",
    "    <li>Dependency Grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words).</li>\n",
    "    <li>Every relation can be represented as a triplet: (relation, governor, dependent)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Example: “Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.”</p>\n",
    "<img src=\"dependency-grammar.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The Tree shows \"submitted\" is the root word of the sentence and is linked by two sub-trees: subject and object</li>\n",
    "    <li>Each subtree is also a dependency tree</li>\n",
    "    <li>This type of tree, parsed top-down gives grammar relation triplets as output that can be used as features for nlp problems like:\n",
    "        <ul>\n",
    "            <li>entity wise sentiment analysis</li>\n",
    "            <li>actor & entrity identification</li>\n",
    "            <li>text classification</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>StanfordCoreNLP (by Stanford NLP Group) and NLTK dependency grammars can be used to generate dependency trees</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part of Speech Tagging</h3>\n",
    "<p>Every word in a sentence is associated with a part of speech (pos) tag.</p>\n",
    "<p>Parts of Speech:</p>\n",
    "<ul>\n",
    "    <li>Nouns</li>\n",
    "    <li>Verbs</li>\n",
    "    <li>Adjectives</li>\n",
    "    <li>Adverbs</li>\n",
    "    <li>Etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rex', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('running', 'VBG'),\n",
       " ('brown', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " (',', ','),\n",
       " ('was', 'VBD'),\n",
       " ('panting', 'VBG'),\n",
       " ('loudly', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "sample_text = \"Rex, the running brown dog, was panting loudly.\"\n",
    "\n",
    "tokens = word_tokenize(sample_text)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A. Word Sense Disambiguation:</h3>\n",
    "<p>Some words in a language have multiple meanings according to their usage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tokenizing/POS tagging cannot detect the difference between these two uses of 'book': \n",
      "I am going to book a flight to PDX.\n",
      "I am going to read this book on my flight to PDX.\n",
      "\n",
      "Example 1 'book': ('book', 'NN')\n",
      "\n",
      "Example 2 'book': ('book', 'NN')\n"
     ]
    }
   ],
   "source": [
    "book1 = \"I am going to book a flight to PDX.\"\n",
    "book2 = \"I am going to read this book on my flight to PDX.\"\n",
    "\n",
    "print(\"Basic tokenizing/POS tagging cannot detect the difference \" +\n",
    "      \"between these two uses of 'book': \", book1, book2, sep=\"\\n\",\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "book1_tokens = word_tokenize(book1)\n",
    "book1_tagged_tokens = pos_tag(book1_tokens)\n",
    "print(\"Example 1 'book':\", book1_tagged_tokens[4], end=\"\\n\\n\")\n",
    "\n",
    "book2_tokens = word_tokenize(book2)\n",
    "book2_tagged_tokens = pos_tag(book2_tokens)\n",
    "print(\"Example 2 'book':\", book2_tagged_tokens[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Important NLP Uses of POS Tagging:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>B. Improving word-based features:</h3>\n",
    "<p>A learning model could learn different contexts of a word - but if the part of speech tag is linked with it, the context is preserved.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>C. Normalization and Lemmatization:</h3>\n",
    "<p>POS tags are the basis of the lemmatization process for converting a word into its base form (lemma)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>D. Efficient stopword removal:</h3>\n",
    "<p>POS tags are also useful in efficient removal of stopwords</p>\n",
    "<p>Some tags always define low-frequency/lower importance words. Ex:</p>\n",
    "<ul>\n",
    "    <li>IN (Preposition or Subordinating Conjunction): \"within\", \"upon\", ... (</li>\n",
    "    <li>CD (Cardinal Number): \"one\", \"two\", ...</li>\n",
    "    <li>MD (Modal): \"may\", \"must\", ...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.2 Entity Extraction (Entities as features)</h1>\n",
    "<p>Entities are the most important chunks of a sentence - noun/verb phrases. Entity detection algorithms usually use rule-based parsing, dictionary lookups, pos tagging, and dependency parsing.</p>\n",
    "<h3>Key NLP Entity Detection Methods:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A. Named Entity Recognition (NER)</h3>\n",
    "<p>The process of detecting named entities from text.</p>\n",
    "<p><strong>Sentence:</strong> \"Sergey Brin, the manager of Google Inc. is walking the streets of New York.\"</p>\n",
    "<p><strong>Named Entities:</strong> (\"person\": \"Sergey Brin\"), (\"org\": \"Google Inc\"), (\"location\": \"New York\")</p>\n",
    "<p>NER model consists of three blocks:</p>\n",
    "<ul>\n",
    "    <li><strong>Noun phrase identification:</strong> extracting the noun phrases using dependency parsing and POS tagging</li>\n",
    "    <li><strong>Phrase Classification:</strong> all extracted noun phrases are classified into respective categories. Resources:\n",
    "    <ul>\n",
    "        <li>Google Maps API - location disambiguation</li>\n",
    "        <li>Wikipedia - person/company names</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li><strong>Entity disambiguation:</strong> It is possible that entities may be misclassified, so creating a validation layer may be useful.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>B. Topic Modeling</h3>\n",
    "<p>A Process of automatically identifying the topics present in a text corpus. Derives hidden patterns among words in the corpus in an unsupervised manner.</p>\n",
    "<ul>\n",
    "    <li><strong>Healthcare:</strong> \"health\", \"doctor\", \"patient\", \"hospital\"</li>\n",
    "    <li><strong>Farming:</strong> \"farm\", \"crops\", \"wheat\"</li>\n",
    "</ul>\n",
    "<p>Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.060*\"driving\" + 0.060*\"may\" + 0.060*\"suggest\" + 0.060*\"stress\" + 0.060*\"that\" + 0.060*\"pressure.\" + 0.060*\"and\" + 0.060*\"Doctors\" + 0.060*\"blood\" + 0.060*\"increased\"'), (1, '0.089*\"to\" + 0.051*\"My\" + 0.051*\"my\" + 0.051*\"sister\" + 0.051*\"sugar,\" + 0.051*\"consume.\" + 0.051*\"is\" + 0.051*\"Sugar\" + 0.051*\"but\" + 0.051*\"have\"'), (2, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"my\" + 0.053*\"My\" + 0.053*\"a\" + 0.053*\"father\" + 0.053*\"dance\" + 0.053*\"time\" + 0.053*\"spends\" + 0.053*\"lot\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "# Create the term dictionary of the corpus, where every unique\n",
    "# term is assigned an index\n",
    "# Ex: (0, \"Sugar\"), (1, \"is\"), (2, \"bad\") ...\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Convert the list of documents (corpus) into a Document Term \n",
    "# Matrix using the dictionary prepared above\n",
    "# doc2bow creates bag-of-words representation:\n",
    "# list of (word_id, word_frequency)\n",
    "# [(0, 1), (1, 1), (2, 1), (3, 2), ...]\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Create the object for the LDA model using the gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Run and train LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3,\n",
    "               id2word=dictionary, passes=50)\n",
    "\n",
    "# Results\n",
    "print(ldamodel.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>C. N-Grams as Features</h3>\n",
    "<p>A combination of N words together are called N-Grams. N-Grams (N > 1) are generally more informative than words (Unigrams) as features. Bigrams (N = 2) are considered the most important features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'sentence']\n",
      "['sentence', 'will']\n",
      "['will', 'be']\n",
      "['be', 'turned']\n",
      "['turned', 'into']\n",
      "['into', 'bigrams']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('this', 'sentence'),\n",
       " ('sentence', 'will'),\n",
       " ('will', 'be'),\n",
       " ('be', 'turned'),\n",
       " ('turned', 'into'),\n",
       " ('into', 'bigrams')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        output.append(tuple(words[i:i+n]))\n",
    "        print(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "sample_text = \"this sentence will be turned into bigrams\"\n",
    "generate_ngrams(sample_text, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.3 Statistical Features</h1>\n",
    "<p>Text data can also be quantified directly into numbers using several techniques.</p>\n",
    "<h3>A. Term Frequency - Inverse Document Frequency (TF - IDF)</h3>\n",
    "<p>TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert text documents into vector models on the basis of occurence of words in the documents without considering the exact ordering.</p>\n",
    "<p><strong>EX -</strong> there is a dataset of N text documents. In any document \"D\", TF and IDF are defined as:</p>\n",
    "<ul>\n",
    "    <li>Term Frequency (TF) - TF for a term \"t\" is defined as the count of a term \"t\" in a document \"D\"</li>\n",
    "    <li>Inverse Document Frequency (IDF) - IDF for a term \"t\" is defined as logarithm of ratio of total documents available in the corpus and number of documents containing \"t\".</li>\n",
    "    <li>TF IDF: the TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula:<img src=\"TF-IDF-formula.png\"></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.274634427112\n",
      "  (0, 2)\t0.274634427112\n",
      "  (0, 7)\t0.464996505949\n",
      "  (0, 4)\t0.464996505949\n",
      "  (0, 3)\t0.464996505949\n",
      "  (0, 6)\t0.35364182827\n",
      "  (0, 1)\t0.274634427112\n",
      "  (1, 8)\t0.364544396761\n",
      "  (1, 2)\t0.364544396761\n",
      "  (1, 6)\t0.469417284322\n",
      "  (1, 1)\t0.364544396761\n",
      "  (1, 0)\t0.617227317565\n",
      "  (2, 8)\t0.412858572062\n",
      "  (2, 2)\t0.412858572062\n",
      "  (2, 1)\t0.412858572062\n",
      "  (2, 5)\t0.699030327257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d1 = \"This is a sort of long sample document.\"\n",
    "d2 = \"This is another sample document.\"\n",
    "d3 = \"This is a random document.\"\n",
    "\n",
    "obj = TfidfVectorizer()\n",
    "corpus = [d1, d2, d3]\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of the word at index j in document i.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>B. Count/Density/Readability Features</h3>\n",
    "<p>Count or Density based features can also be used in models and analysis. These features might seem trivial but they have a great impact in learning models. Some of the features are: </p>\n",
    "<ul>\n",
    "    <li>Word Count</li>\n",
    "    <li>Sentence Count</li>\n",
    "    <li>Punctuation Count</li>\n",
    "    <li>Industry Specific Word Count</li>\n",
    "</ul>\n",
    "<p>Other types of measures include readability measures such as:</p>\n",
    "<ul>\n",
    "    <li>Syllable Counts</li>\n",
    "    <li>Smog Index</li>\n",
    "    <li>Flesch Reading Ease</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.4 Word Embedding (text vectors)</h1>\n",
    "<p>Representing words as vectors. Aims to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks</p>\n",
    "<p>Word2Vec GloVe are two popular models to create word embedding of a text. Takes a corpus as input and produces word vectors as output.</p>\n",
    "<p>Word2Vec is composed of a preprocessing module, shallow neural network model (Continuous Bag of Words), and another shallow neural network (skip-gram). These models are widely used for NLP problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the Text Vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0556528586891\n",
      "[  1.04084518e-03  -1.77087018e-03  -4.61314805e-03  -3.06192669e-03\n",
      "  -1.42174272e-03  -2.12180754e-03  -3.43434309e-04   1.28958223e-03\n",
      "   2.03153747e-03   2.86165567e-04  -2.45753489e-03  -4.70468448e-03\n",
      "  -4.27514518e-04  -1.77422364e-03   3.74865299e-03   3.30105773e-03\n",
      "   4.28414717e-03  -4.15968476e-03  -3.81730171e-03   3.44632915e-03\n",
      "  -4.37509082e-03  -3.91984126e-03   4.96243173e-03   2.10108631e-03\n",
      "  -4.47424827e-04   3.18493135e-03  -4.91256453e-03   3.60128330e-03\n",
      "   3.76950120e-05  -4.82656760e-03  -6.73248258e-04  -7.62612792e-04\n",
      "   3.35885887e-03  -4.64811642e-03   5.33412036e-04  -1.91591599e-03\n",
      "  -3.18837818e-03  -3.16291675e-03  -3.00405663e-03   1.34144025e-03\n",
      "  -2.76611256e-03   9.11801762e-04   2.10785563e-03   1.06058235e-03\n",
      "  -1.23878563e-04   4.94708493e-03  -3.91837023e-03   3.36146983e-03\n",
      "  -3.94387683e-03  -1.05841365e-03   2.58075166e-03   2.48690927e-03\n",
      "   4.98169975e-04  -4.48935665e-03  -4.39680321e-03   1.05393201e-03\n",
      "   2.51843012e-03  -4.37599592e-05  -3.62810865e-03   4.30254918e-03\n",
      "   3.01068602e-03  -3.92695237e-03  -2.21937336e-03   2.55270186e-03\n",
      "   4.70036129e-03   1.09749567e-03   1.58755854e-03  -3.36437603e-03\n",
      "   2.53218738e-03  -6.30070048e-04  -3.64846201e-03   4.09282465e-03\n",
      "  -2.45678675e-04  -4.13301820e-03  -3.47881787e-03   2.38344236e-03\n",
      "  -3.61276744e-03  -3.43493768e-03  -1.86957314e-03  -2.30193231e-03\n",
      "   3.50016076e-03  -3.99981719e-03  -3.69094056e-03  -1.05634972e-03\n",
      "   6.38320635e-05   4.65963781e-03  -2.60391535e-04   4.51387977e-03\n",
      "  -4.84250486e-03  -1.37052580e-03  -1.85422983e-03   4.35631955e-03\n",
      "  -1.99979846e-03   2.24666391e-03  -8.10318510e-04   3.59382643e-03\n",
      "   3.01962346e-03   2.11843220e-03  -2.39110552e-03  -4.64166980e-04]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"data\", \"science\"], [\"learning\", \"science\", \"data\", \"analytics\"],\n",
    "             [\"machine\", \"learning\"], [\"deep\", \"learning\"]]\n",
    "\n",
    "# Train the model on your corpus\n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity(\"data\", \"science\"))\n",
    "print(model[\"learning\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
