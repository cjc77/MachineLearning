{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2 Text Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing Is Comprised Mainly of Three Steps:</h1>\n",
    "<ul>\n",
    "    <li>Noise Removal</li>\n",
    "    <li>Lexicon Normalization</li>\n",
    "    <li>Object Standardization</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2.1 Noise Removal</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove Noisy Words from Text Using Standard Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\"]\n",
    "sample_text = \"This is some sample text.\"\n",
    "\n",
    "def remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    scrubbed = \" \".join([word.lower() for word in words if word.lower()\n",
    "                         not in noise_list])\n",
    "    return scrubbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some sample text.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_noise(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove Noisy Words from Text Using Regular Expressions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "reg_ex = \"#[\\w]*\"\n",
    "sample_text = \"This is a sentence with a #hashtag\"\n",
    "\n",
    "def remove_noise_regex(input_text, regex_pattern):\n",
    "    reg_iter = re.finditer(regex_pattern, input_text)\n",
    "#     for i in reg_iter:\n",
    "#         input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    input_text = [re.sub(i.group().strip(), '', input_text)\n",
    "                  for i in reg_iter]\n",
    "    return ''.join(input_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sentence with a '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_noise_regex(sample_text, reg_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2.2 Lexicon Normalization</h1>\n",
    "<p>The same word can have multiple representations, Ex: \"play\", \"player\", \"played\", \"plays\", \"playing\".</p>\n",
    "<p>Normalization: Convert all disparities of a word into a normalized form (aka lemma).</p>\n",
    "<p>This converts high dimensional features into a low dimensional space (1 feature).</p>\n",
    "<h3>Common Lexicon Normalization Practices:</h3>\n",
    "<ul>\n",
    "    <li>Stemming: rule-based process of stripping suffixes (\"ing\", \"ly\", \"es\", \"s\", etc)</li>\n",
    "    <li>Lemmatization: Organized, step-by-step procedure of obtaining the root form of a word. Uses vocabulary (dictionary importance) and morphological analysis (word structure and grammar relations).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NLTK lemmatization and stemming</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# NOTE: OPTIONAL - this is just me goofing around\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('climbing', 'VBG'),\n",
       " ('climbs', 'NNS'),\n",
       " ('climber', 'NN'),\n",
       " ('climbers', 'NNS')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climbing = \"climbing\"\n",
    "climbs = \"climbs\"\n",
    "climber = \"climber\"\n",
    "climbers = \"climbers\"\n",
    "\n",
    "# NOTE: OPTIONAL - this is just me goofing around\n",
    "climb_ls = ' '.join([climbing, climbs, climber, climbers])\n",
    "climb_tokenized = pos_tag(word_tokenize(climb_ls))\n",
    "climb_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiplying', 'VBG'),\n",
       " ('multiplies', 'NNS'),\n",
       " ('multiplier', 'JJR'),\n",
       " ('multipliers', 'NNS')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplying = \"multiplying\"\n",
    "multiplies = \"multiplies\"\n",
    "multiplier = \"multiplier\"\n",
    "multipliers = \"multipliers\"\n",
    "\n",
    "# NOTE: OPTIONAL - this is just me goofing around\n",
    "mult_ls = ' '.join([multiplying, multiplies, multiplier,\n",
    "                    multipliers])\n",
    "mult_tokenized = pos_tag(word_tokenize(mult_ls))\n",
    "mult_tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lemmatize</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climb\n",
      "climb\n",
      "climber\n",
      "climber\n",
      "+++++++++++++++++++++++++++++++++++\n",
      "multiply\n",
      "multiply\n",
      "multiplier\n",
      "multiplier\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(climbing, \"v\"))\n",
    "print(lem.lemmatize(climbs, \"v\"))\n",
    "print(lem.lemmatize(climber, \"n\"))\n",
    "print(lem.lemmatize(climbers, \"n\"))\n",
    "print(\"+++++++++++++++++++++++++++++++++++\")\n",
    "print(lem.lemmatize(multiplying, \"v\"))\n",
    "print(lem.lemmatize(multiplies, \"v\"))\n",
    "print(lem.lemmatize(multiplier, \"n\"))\n",
    "print(lem.lemmatize(multiplier, \"n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Stem</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climb\n",
      "climb\n",
      "climber\n",
      "climber\n",
      "+++++++++++++++++++++++++++++++++++\n",
      "multipli\n",
      "multipli\n",
      "multipli\n",
      "multipli\n"
     ]
    }
   ],
   "source": [
    "print(stem.stem(climbing))\n",
    "print(stem.stem(climbs))\n",
    "print(stem.stem(climber))\n",
    "print(stem.stem(climbers))\n",
    "print(\"+++++++++++++++++++++++++++++++++++\")\n",
    "print(stem.stem(multiplying))\n",
    "print(stem.stem(multiplies))\n",
    "print(stem.stem(multiplier))\n",
    "print(stem.stem(multiplier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2.3 Object Standardization</h1>\n",
    "<p>Text data can contain words or phrases that aren't in standard lexical dictionaries - these words will often not be recognized.</p>\n",
    "<p>Examples:</p>\n",
    "<ul>\n",
    "    <li>acronyms</li>\n",
    "    <li>hashtags</li>\n",
    "    <li>colloquial slangs</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {\"rt\": \"retweet\", \"dm\": \"direct message\",\n",
    "               \"awsm\": \"awesome\", \"luv\": \"love\"}\n",
    "sample_text = \"I'm going to rt that dm because it was awsm I loved it\"\n",
    "\n",
    "def lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm going to retweet that direct message because it was awesome I loved it\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_words(sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
